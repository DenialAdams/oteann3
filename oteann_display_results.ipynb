{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook reads one (or more) OTEANN results file(s) and vizualises them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "def get_unique_chars(filename):\n",
    "    unique_chars = set(char for line in codecs.open(filename, encoding=\"UTF-8\") for char in line)\n",
    "    return unique_chars\n",
    "\n",
    "def display_orthography_letters(config):\n",
    "\n",
    "    languages = config['languages']\n",
    "\n",
    "    for lang in languages:\n",
    "        filename = config['root_dir'] + '/' + config['subdatasets_dir'] + '/' + lang + '_' + config['subdataset']\n",
    "        filename_g = filename + '_graphemes.tmp'\n",
    "        filename_p = filename + '_phonemes.tmp'\n",
    "\n",
    "        df = pd.read_csv(filename)\n",
    "\n",
    "        df.Word.to_csv(filename_g, index=False, header=None)\n",
    "        df_g = pd.read_csv(filename_g)\n",
    "\n",
    "        df.Pronunciation.to_csv(filename_p, index=False, header=None)\n",
    "        df_p = pd.read_csv(filename_p)\n",
    "\n",
    "        unique_graphemes = get_unique_chars(filename_g)\n",
    "        unique_phonemes = get_unique_chars(filename_p)\n",
    "        \n",
    "        os.remove(filename_g)\n",
    "        os.remove(filename_p)\n",
    "        print('%s, n_phonemes:%d, n_graphemes:%d' % (\n",
    "            lang, len(unique_phonemes)-1, len(unique_graphemes)-1)) # -1 is for ''\\n'\n",
    "        print(\"graphemes:\", sorted(unique_graphemes))\n",
    "        print(\"phonemes:\", sorted(unique_phonemes))\n",
    "        print(\"-------------------------------\")\n",
    "#display_orthography_letters(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These first configuration parameters are hyperparameters that we will need to tune\n",
    "CONFIG = {            \n",
    "    'n_train': 0,\n",
    "}\n",
    "\n",
    "ROOT_DIR = os.getcwd()\n",
    "\n",
    "# These other configuration parameters will not be tuned\n",
    "def extend_config(config): \n",
    "    config['languages'] = ['ent', 'eno','ar', 'br', 'de', 'en', 'eo', 'es', 'fi', 'fr', 'fro', 'it', 'ko', 'nl', 'pt', 'ru', 'sh', 'tr', 'zh']\n",
    "    config['tasks'] = ['write', 'read']\n",
    "    #config['languages'] = ['ent', 'eno', 'br', 'fr']\n",
    "    #config['languages'] = ['ent']    \n",
    "    config['n_test'] = 1000\n",
    "    config['n_samples'] = config['n_train'] + config['n_test']\n",
    "    config['label'] = 'oteann' + '_' + str(config['n_samples'])\n",
    "    config['subdatasets_dir'] = 'subdatasets'\n",
    "    config['subdataset'] = 'wikt_samples.csv' # postfix from fonetik.fr \n",
    "    config['root_dir'] = os.getcwd()    \n",
    "    config['trial_dir'] = os.getcwd() \n",
    "    config['trial_filename'] = config['trial_dir'] + '/' + config['label']\n",
    "    config['results_filename'] = config['trial_filename'] + '_results.csv'\n",
    "    return config\n",
    "        \n",
    "config = extend_config(CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_textual_results(config):\n",
    "    df_results = pd.read_csv(config['results_filename']) \n",
    "    # display the \"write\" results of the different languages\n",
    "    # the best writing-system is on the first line\n",
    "    # the worst writing-system is on the last line\n",
    "    print('scoring of the orthograpies with respect to \"write\" task:')\n",
    "    print(df_results[df_results.task == 'write'].sort_values('test_accuracy', ascending=False))\n",
    "    print()\n",
    "    # display the \"read\" results of the different languages\n",
    "    # the best reading-system is on the first line\n",
    "    # the worst reading-system is on the last line\n",
    "    print('scoring of the orthograpies with respect to \"read\" task:')\n",
    "    print(df_results[df_results.task == 'read'].sort_values('test_accuracy', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_text_results(config):\n",
    "    df_results = pd.read_csv(config['results_filename']) \n",
    "    # display the \"write\" results oe the different languages\n",
    "    print(\"|-----------|------------|------------|\")\n",
    "    print(\"|orthography| write score| read score |\")\n",
    "    print(\"|-----------|------------|------------|\")\n",
    "    for lang in df_results.lang.unique():\n",
    "        dfl = df_results[df_results.lang == lang]\n",
    "        print(\"|    %3s    | %04.1f ± %2.1f | %04.1f ± %2.1f |\" % (\n",
    "              lang.ljust(3),\n",
    "              round(dfl[dfl.task == 'write'].mean()*100, 1),\n",
    "              round(dfl[dfl.task == 'write'].std()*100, 1),\n",
    "              round(dfl[dfl.task == 'read'].mean()*100, 1),\n",
    "              round(dfl[dfl.task == 'read'].std()*100, 1))\n",
    "             )\n",
    "        if lang == 'eno':\n",
    "            print(\"|-----------|------------|------------|\")\n",
    "    print(\"|-----------|------------|------------|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_latex_results(config):\n",
    "\n",
    "    print('\\\\begin{figure}[h]')\n",
    "    print('\\centering')\n",
    "    print('\\captionsetup{justification=centering}')\n",
    "    print('\\\\begin{floatrow}')\n",
    "    print('\\\\capbtabbox{%')\n",
    "    print('\\\\begin{tabular}{c c c}')\n",
    "    print('\\\\hline')\n",
    "    print('\\\\textbf{Language} & \\\\textbf{Write} & \\\\textbf{Read}\\\\\\\\')\n",
    "    print('\\\\hline\\hline')\n",
    "    \n",
    "    df_results = pd.read_csv(config['results_filename']) \n",
    "    # display the \"write\" results oe the different languages\n",
    "    n = config['n_train']\n",
    "    \n",
    "    for lang in df_results.lang.unique():\n",
    "        dfl = df_results[df_results.lang == lang]\n",
    "        print(\"%s & %4.1f ± %2.1f & %4.1f ± %2.1f \\\\\\\\\\\\hline\" % (\n",
    "              lang.ljust(3),\n",
    "              round(dfl[dfl.task == 'write'].mean()*100, 1),\n",
    "              round(dfl[dfl.task == 'write'].std()*100, 1),\n",
    "              round(dfl[dfl.task == 'read'].mean()*100, 1),\n",
    "              round(dfl[dfl.task == 'read'].std()*100, 1))\n",
    "             )\n",
    "        if lang == 'eno':\n",
    "            print('\\\\hline')  \n",
    "    print('\\\\end{tabular}')\n",
    "    print('}{%')\n",
    "    print('  \\\\caption{Phonemic transparency scores. \\\\newline ' + \n",
    "          '(OTEANN trained with $' + f'{n:,}' + '$ samples)}%')\n",
    "    print('  \\\\label{tab:tabular_results_' + str(config['n_samples']) + '}')\n",
    "    print('}')\n",
    "    print('\\\\ffigbox{%')\n",
    "    print('\\\\includegraphics[width=7.5cm]{oteann_' + \n",
    "          str(config['n_samples']) + '_results.png}%')\n",
    "    print('}{%')\n",
    "    print('  \\\\caption{Scatterplot of the mean scores. \\\\newline ' + \n",
    "          '(OTEANN trained with $' + f'{n:,}' + '$ samples)}%')\n",
    "    print('  \\\\label{fig:figure_results_' + str(config['n_samples']) + '}')\n",
    "    print('}')\n",
    "    print('\\\\end{floatrow}')\n",
    "    print('\\\\end{figure}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_barplot_results(config):\n",
    "    \n",
    "    df = pd.read_csv(config['results_filename'])\n",
    "    \n",
    "    sns.set(style=\"whitegrid\")\n",
    "    current_palette = sns.color_palette()\n",
    "    for task in config['tasks']:\n",
    "        df_o = df[df.task==task].sort_values(by='lang')\n",
    "        df_o = df_o[['task', 'lang', 'test_accuracy']]\n",
    "        df_o['test_accuracy']=df_o['test_accuracy']*100\n",
    "        df_o = round(df_o.groupby('lang', as_index=False).mean())\n",
    "        sns.palplot(current_palette)\n",
    "        ax = sns.barplot(x=\"lang\", y=\"test_accuracy\",\n",
    "                         data=df_o, palette=current_palette)\n",
    "        # add the accuracy number on the top of each bar\n",
    "        i=0\n",
    "        for index, row in df_o.iterrows():\n",
    "            ax.text(i, row.test_accuracy+1, str(round(row.test_accuracy)), color='black', ha=\"center\")\n",
    "            i+=1\n",
    "        plt.title(task.capitalize())\n",
    "        plt.ylim(0, 100)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_boxplot_results(config):\n",
    "    \n",
    "    df = pd.read_csv(config['results_filename'])\n",
    "    \n",
    "    sns.set(style=\"whitegrid\")\n",
    "    current_palette = sns.color_palette()\n",
    "    for task in config['tasks']:\n",
    "        df_o = df[df.task==task].sort_values(by='lang')\n",
    "        df_o = df_o[['task', 'lang', 'test_accuracy']]\n",
    "        df_o['test_accuracy']=df_o['test_accuracy']*100\n",
    "        #df_o = df_o.groupby('lang', as_index=False).mean()\n",
    "        sns.palplot(current_palette)\n",
    "        ax = sns.boxplot(x=\"lang\", y=\"test_accuracy\",\n",
    "                         data=df_o, palette=current_palette)\n",
    "        # add the accuracy number on the top of each bar\n",
    "        i=0\n",
    "        for index, row in df_o.iterrows():\n",
    "            #ax.text(i, row.test_accuracy+1, str(round(row.test_accuracy)), color='black', ha=\"center\")\n",
    "            i+=1\n",
    "        plt.title(task.capitalize())\n",
    "        plt.ylim(0, 100)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_config(config):\n",
    "    n = config['n_train']\n",
    "    print('n_train:', f'{n:,}')\n",
    "    df_results = pd.read_csv(config['results_filename'])\n",
    "    n_tasks = len(df_results['task'].unique())\n",
    "    n_langs = len(df_results['lang'].unique())\n",
    "    print('result file:')\n",
    "    print('* n_tasks: %d' % n_tasks)\n",
    "    print('* n_langs: %d' % n_langs)\n",
    "    print('* n_rows: %d' % df_results.shape[0])\n",
    "    if n_tasks > 0 and n_langs > 0:\n",
    "        nb_episodes = float(df_results.shape[0]) / (n_tasks * n_langs)\n",
    "        print('* nb_episodes: %.2f' % nb_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_point(x, y, val, ax):\n",
    "    a = pd.concat({'x': x, 'y': y, 'val': val}, axis=1)\n",
    "    for i, point in a.iterrows():\n",
    "        ax.text(point['x']+2, point['y'], str(point['val']))\n",
    "\n",
    "def display_scatterplot_results(config):\n",
    "    \n",
    "    df_results = pd.read_csv(config['results_filename']) \n",
    "    \n",
    "    df_results = df_results[~df_results['lang'].str.contains('eno')]\n",
    "    df_results = df_results[~df_results['lang'].str.contains('ent')]\n",
    "    \n",
    "    df_results = df_results.groupby(['lang','task'], as_index=False).mean()\n",
    "    \n",
    "    df_w = df_results[df_results.task=='write']\n",
    "    df_w.rename(columns={\"task\": \"write\"})\n",
    "    df_r = df_results[df_results.task=='read']\n",
    "    df_r.rename(columns={\"task\": \"read\"})\n",
    "\n",
    "    df_res = df_w.merge(df_r, left_on='lang', right_on='lang')\n",
    "    df_res = df_res.rename(columns={\"test_accuracy_x\": \"write_accuracy\", \"test_accuracy_y\": \"read_accuracy\"})\n",
    "    df_res['write_accuracy'] = df_res['write_accuracy'] * 100\n",
    "    df_res['read_accuracy'] = df_res['read_accuracy'] * 100\n",
    "\n",
    "    sns.set_theme(color_codes=True)\n",
    "    \n",
    "    ax = sns.lmplot(\n",
    "        data=df_res, # Data source\n",
    "        x='write_accuracy',\n",
    "        y='read_accuracy',\n",
    "        fit_reg=False, # Don't fix a regression line\n",
    "        height=5,\n",
    "        aspect=1) # size and dimension\n",
    "    \n",
    "    #plt.title('Transparency of the orthographies (with %d training samples)' % config['n_train'])\n",
    "    # Set x-axis label\n",
    "    plt.xlabel('Write accuracy')\n",
    "    # Set y-axis label\n",
    "    plt.ylabel('Read accuracy')\n",
    "    \n",
    "    label_point(df_res.write_accuracy, df_res.read_accuracy, df_res.lang, plt.gca())\n",
    "    \n",
    "    ax.set(xlim=(-1.5, 101.5))\n",
    "    ax.set(ylim=(-1.5, 101.5))        \n",
    "\n",
    "    #plt.subplots_adjust(left=-0.1, bottom=-0.1, right=1.05, top=1.05)\n",
    "    plt.tight_layout()\n",
    "    figure_file = 'oteann_' + str(config['n_samples']) + '_results.png'\n",
    "    plt.show()\n",
    "    plt.savefig(figure_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_single_scatterplot_results(config, n_trains):\n",
    "    \n",
    "    sns.set(rc={'figure.figsize':(6.5, 6.5)})\n",
    "    \n",
    "    df_res = pd.DataFrame(columns=['n_train', 'orthography', 'write_accuracy', 'read_accuracy'])    \n",
    "    \n",
    "    for n_train in n_trains:\n",
    "        config['n_train'] = n_train\n",
    "        config = extend_config(config)\n",
    "        df_results = pd.read_csv(config['results_filename']) \n",
    "        df_results = df_results.rename(columns={'lang': 'orthography'})\n",
    "\n",
    "        df_results = df_results.groupby(['orthography','task'], as_index=False).mean()\n",
    "\n",
    "        df_w = df_results[df_results.task=='write']\n",
    "        df_w.rename(columns={\"task\": \"write\"})\n",
    "        df_r = df_results[df_results.task=='read']\n",
    "        df_r.rename(columns={\"task\": \"read\"})\n",
    "        \n",
    "        df_res_i = df_w.merge(df_r, left_on='orthography', right_on='orthography')\n",
    "        df_res_i = df_res_i.rename(columns={\"test_accuracy_x\": \"write_accuracy\", \"test_accuracy_y\": \"read_accuracy\"})\n",
    "        df_res_i = df_res_i.drop(columns=['task_x', 'task_y'])\n",
    "        df_res_i['n_train'] = n_train\n",
    "\n",
    "        df_res = df_res.append(df_res_i)\n",
    "        \n",
    "    ax = sns.scatterplot(\n",
    "        data=df_res, # Data source\n",
    "        x='write_accuracy',\n",
    "        y='read_accuracy',\n",
    "        hue='orthography',\n",
    "        size='n_train',\n",
    "        alpha=0.70,\n",
    "        legend='full',\n",
    "        size_order=[1000, 2000, 3000, 5000, 10000]\n",
    "    )\n",
    "\n",
    "    #plt.title('Transparency of the orthographies (with %d training samples)' % config['n_train'])\n",
    "    # Set x-axis label\n",
    "    plt.xlabel('Write accuracy')\n",
    "    # Set y-axis label\n",
    "    plt.ylabel('Read accuracy')\n",
    "\n",
    "    #df_res_last = df_res[df_res.n_train == n_trains[-1]]\n",
    "    #label_point(df_res_last.write_accuracy, df_res_last.read_accuracy, df_res_last.lang, plt.gca())\n",
    "\n",
    "    ax.set(xlim=(-0.01, 1.015))\n",
    "    ax.set(ylim=(-0.01, 1.015))\n",
    "    #plt.figure(figsize=(150, 150))\n",
    "    \n",
    "    \n",
    "    #handles, labels = ax.legend_elements(prop=\"lang\", alpha=0.6)\n",
    "    #legend2 = ax.legend(handles, labels, loc=\"upper right\", title=\"Orthography\")\n",
    "\n",
    "    plt.subplots_adjust(left=-0.2, bottom=-0.1, right=1.05, top=1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.legend(bbox_to_anchor=(1.01, 1),borderaxespad=0)\n",
    "    figure_file = 'oteann_' + str(config['n_samples']) + '_results.png'\n",
    "    plt.show()\n",
    "    plt.savefig(figure_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trains = [1000, 2000, 3000, 5000, 10000]\n",
    "\n",
    "for n_train in n_trains:\n",
    "    config['n_train'] = n_train\n",
    "    config = extend_config(config)\n",
    "display_single_scatterplot_results(config, n_trains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results = [1000, 2000, 3000, 5000, 10000]\n",
    "\n",
    "for n_train in results:\n",
    "    print('########################################')\n",
    "    config = { \n",
    "        'n_train': n_train\n",
    "    }\n",
    "    config = extend_config(config)\n",
    "    display_scatterplot_results(config)\n",
    "    print('########################################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results = [1000, 2000, 3000, 5000, 10000]\n",
    "\n",
    "for n_train in results:\n",
    "    print('########################################')\n",
    "    config = { \n",
    "        'n_train': n_train\n",
    "    }\n",
    "    config = extend_config(config)\n",
    "    #display_textual_results(config)\n",
    "    display_config(config)\n",
    "    display_text_results(config)\n",
    "    display_barplot_results(config)\n",
    "    display_boxplot_results(config)\n",
    "    display_latex_results(config)\n",
    "    display_scatterplot_results(config)\n",
    "    print('########################################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarizes_dataset(language, debug=False):\n",
    "    if debug:\n",
    "        print('lang:', language)\n",
    "    result = {\n",
    "        'language' : language\n",
    "    }\n",
    "    df = pd.read_csv('subdatasets/'+ language +'_wikt_samples.csv')\n",
    "    if debug:\n",
    "        print('df.shape:', df.shape)\n",
    "        print(df.head(3))\n",
    "    word_chars = ''.join(df['Word'].values)\n",
    "    pron_chars = ''.join(df['Pronunciation'].values)\n",
    "    wtoi = { ch:i for i,ch in enumerate(word_chars) }\n",
    "    ptoi = { ch:i for i,ch in enumerate(pron_chars) }\n",
    "    if debug:\n",
    "        print('wtoi:', wtoi)\n",
    "        print('')\n",
    "        print('ptoi:', ptoi)\n",
    "    result['n_rows'] = df.shape[0]\n",
    "    samples = df.shape[0]\n",
    "    result['n_phonemes'] = len(ptoi)\n",
    "    result['n_graphemes'] = len(wtoi)\n",
    "    df['Word_len'] = df['Word'].str.len()\n",
    "    df['Pron_len'] = df['Pronunciation'].str.len()\n",
    "    result['phonemes_mean_len'] = int(df['Pron_len'].mean()*10)/10\n",
    "    result['phonemes_mean_std'] = int(df['Pron_len'].std()*10)/10\n",
    "    result['graphemes_mean_len'] = int(df['Word_len'].mean()*10)/10\n",
    "    result['graphemes_mean_std'] = int(df['Word_len'].std()*10)/10\n",
    "    \n",
    "    latex_str = language + ' & ' +  \\\n",
    "                f'{samples:,}' + ' & ' +  \\\n",
    "                str(result['n_phonemes']) + ' & ' +  \\\n",
    "                str(result['n_graphemes']) + ' & ' +  \\\n",
    "                str(result['phonemes_mean_len']) + ' ± ' + str(result['phonemes_mean_std']) + ' & ' +  \\\n",
    "                str(result['graphemes_mean_len']) + ' ± ' + str(result['graphemes_mean_std']) + ' \\\\\\\\' \n",
    "    print(latex_str)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_datasets = []\n",
    "languages = ['ar', 'br', 'de', 'en', 'eo', 'es', 'fi', 'fr', 'fro', \n",
    "             'it', 'ko', 'nl', 'pt', 'ru', 'sh', 'tr', 'zh',\n",
    "             'eno', 'ent' ]\n",
    "for lang in languages:\n",
    "    try:\n",
    "        result = summarizes_dataset(lang)\n",
    "        res_datasets.append(result)\n",
    "    except:\n",
    "        print('pb for:', lang)\n",
    "    df_datasets = pd.DataFrame(res_datasets)\n",
    "df_datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
